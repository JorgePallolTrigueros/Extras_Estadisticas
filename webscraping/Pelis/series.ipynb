{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339c7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = ['Sombra y huesos','Loca historia del mundo','Big Mouth', 'Millennium','Vampire Academi','The Rising','INSPECTOR MORSE','Pobre Diablo','Adamas','alice','American Horror Stories','Arte','Caballero Luna','Cantina','Castlevania','Cazar al culpable','City on a Hill','Corona vacia','Culturista asesina','cuphead','Cypher','Dahmer','Dark Winds','Dectetive Endeavour','Dualidad','EL JOVEN MONTALBANO','El Pacificador','Excepcion','From','Gaslit','Hausen','HeMan y los Masters del Universo','hijo bastardo','Inspector Venn','Jovenes en orbita','La Asesina Del Romance','La Casa del Dragon','La Maldicion De Hill House','La novia Gitana','Los Anillos De Poder','Los Bridgerton','Los Gemstone','Los Winchester','Marion','Obi-Wan Kenobi','Ojo De Halcon','Oni La Leyenda Del Dios Del Trueno','Pennyworth','Perimetral','Petra','Por que no le preguntan a Evans','Sandman','sangre y sexo','She-Hulk Abogada Hulka','SurrealEstate','Tekken Linaje','The Boys','The Peripheral','The Serpent Queen','Un asunto privado','Westworld','Zootropolis','willou','don patrol','Boys over love','Chipendale','Yellou Jacket','La historia secreta del comic','the good.fight','depardiu','Chicago Party Aunt','Comando Queer','Reginaldo','joven kidaichi','hunters','Entrevista con el vampiro','Alquimia de almas','Torre','Cafe Minamdang','Habilidad fisica 100','Las Nuevas Aventuras De Batman','La chica de nieve','La gloria','South of Hell','Despiï¿½rtate',' Rachel Gunn','CATWALK ','Belascoaran','the calling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba4cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.filmaffinity.com/es/film826281.html']\n",
      "Conectado correctamente\n",
      "CONTINUE\n",
      "https://www.filmaffinity.com/es/film826281.html\n",
      "22 min.\n",
      "INSERT INTO `pelis`.`pelis2` (`titulo`,`anio`,`duraciones`,`paises`,`direcciones`,`guiones`,\n",
      "`musicas`,`fotografias`,`repartos`,\n",
      "`productoras`,`generos`,`grupos`,`sinopsises`) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
      "Futurama\n",
      "1999\n",
      "22 min.\n",
      "Estados Unidos\n",
      "Christopher Tyng\n",
      "Christopher Tyng\n",
      "T h e   C u r i o s i t y   C o m p a n y ,   2 0 t h   C e n t u r y   F o x   T e l e v i s i o n .     E m i t i d a   p o r :   F O X ,   C o m e d y   C e n t r a l\n",
      "Ahora van las fotos\n",
      "https://pics.filmaffinity.com/futurama-269665233-mmed.jpg\n",
      "Ya acabaron las fotos\n",
      "\n",
      "The Curiosity Company, 20th Century Fox Television.  FOX, Comedy Central\n",
      "Serie de TV. Animación. Comedia. Ciencia ficción | Extraterrestres. Robots. Aventura espacial. Sátira\n",
      "Futurama\n",
      "Serie de TV (1999-2003. 2010-2013. 2023-). 8 temporadas. +140 episodios. Serie de animación creada por Matt Groening (creador también de Los Simpson) y David X. Cohen (guionista también de Los Simpson). Ambientada en la ciudad de \"Nueva Nueva York\" en el año 3000, la serie comienza con Philip J. Fry, un joven repartidor de pizza neoyorquino fracasado y desmotivado que es criogénicamente congelado por accidente la Nochevieja de 1999. Mil años después es descongelado, encontrándose en Nueva Nueva York el 31 de diciembre de 2999. El intento de Fry por escapar de la entonces obligatoria asignación laboral como repartidor termina cuando es contratado en Planet Express, una pequeña compañía de mensajería intergaláctica propiedad de su sobrino lejano, como repartidor. La serie trata sobre las aventuras de Fry y sus colegas cuando viajan por el universo haciendo repartos para Planet Express. (FILMAFFINITY)\n",
      "\n",
      "En mayo de 2023 se anunció una nueva temporada de la serie, a estrenarse en julio.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import pymysql\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import pymysql\n",
    "\n",
    "\n",
    "#lista=['My Last Day','7seeds','acca','Akame ga Kill!','APARI3NCIAS','Assassination Classroom','b','bastard', 'Beastars', 'Black Rock Shooter Dawn Fall', 'blue', 'BTOOOM!','c y t', 'cazadores de demonios', 'Chikyuugai Shounen Shoujo','Cider no You ni Kotoba ga Wakiagaru', 'conan', 'DanMachi', 'Death Note','el conde de Montecisto', 'El hundimiento de Japon', 'El mágico libro de Zero', 'excepción','Fairy Tail', 'fate', 'Getbakers', 'ghost', 'Ghost in the Shell', 'Ghost In The Shell S.A.C. 2nd','Great Pretender', 'gun', 'Gun X Sword', 'gungrave', 'hades', 'Hataraku Saibou','hero', 'hero mask', 'Hi Score Girl', 'Hijos de las ballenas', 'ingress', 'Invasión de altura','Isla de Sombras', 'JoJo no Kimyou na Bouken', 'Kabaneri de la Fortaleza de Hierro','Kakegurui Twin', 'Kakegurui', 'Kotaro vive solo', 'Koukaku Kidoutai', 'La leyenda de los heroes de la galaxia','Le Chevalier DEon Castellano', 'Mahoutsukai no Yome', 'Monster', 'mushi', 'Nana', 'No Game no live', 'Noragami','Omega', 'Owari no Seraph', 'Pantheon', 'Paranoia Agent', 'Perdido', 'Psycho-Pass', 'r', 'Record of Ragnarok','Sagrada Reset', 'Saint Seiya  Meiou Hades Elysion', 'Saint Seiya The Lost Canvas', 'Sakamoto Desu ga','Sangre de Zeus', 'seis manos', 'seiya', 'shadow', 'Shaman King', 'Sono Bisque Doll wa Koi o Suru','Spriggan', 'Steins Gate', 'Super Crooks', 'Sword Art', 'SWORDGAI', 'Tekken Bloodline','The Seven Deadly Sins', 'The Way of the Househusband', 'TIGER & BUNNY', 'Tokyo Ghoul','Tokyo Ghoul Re 2nd Season', 'Uchiage Hanabi, Shita kara Miru ka', 'Ultramarine Magmell','Vampire in the Garden', 'Violet Evergarden', 'xxholic', 'yakusoku no nerverland', 'Saint Seiya','blasck rock', 'Komi-san', 'castlevania', 'DRIFTING DRAGONS', 'EDENS ZERO', 'Hotel', 'yasuke', 'BNA','seven deadley', 'Housing Complex C', 'Konosuba. Un mundo maravilloso', 'Mushishi','The Seven Deadly Sins El rencor de Edimburgo', '1', 'De yakuza a amo de casa', 'relatos','Setective Conan Hanzawa el Culpable', 'Akudama Drive', 'Black Clover', 'Espiritu de lucha','Ousama Ranking', 'Tate no Yuusha no Nariagari', 'family spy', 'Jujutsu Kaisen', 'shadow castle','Summer Time render', 'Gunslinger Girl', 'Boku no Hero Academi'];\n",
    "lista1=[];\n",
    "lista =[\"Futurama\"];\n",
    "escluidos=[];   \n",
    "Lista_de_peliculas=[];\n",
    "\n",
    "\n",
    "escluidos=[];   \n",
    "Lista_de_peliculas=[];\n",
    "\n",
    "#El input del buscador\n",
    "id=\"top-search-input\"\n",
    "\n",
    "# ir a la web https://www.filmaffinity.com/es/search.php?stext=NOMBRE_PELICULA\n",
    "# y coger el primer data-movie-id que aparezca\n",
    "\n",
    "lista_deurl=[]\n",
    "for nombrePeli in lista:\n",
    "    url = \"https://www.filmaffinity.com/es/search.php?stext=\" + nombrePeli\n",
    "    sleep(3)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\") \n",
    "    \n",
    "    try:\n",
    "        peliId = soup.find(\"div\", class_ = \"rate-movie-box\") # Casos de un solo resultado\n",
    "        if(peliId is None):\n",
    "            peliId = soup.find(\"div\", class_ = \"movie-card-1\") # Casos de multiples resultados, cojo la primera peli (Comparar fecha )        \n",
    "        peliIdValue = peliId['data-movie-id'];\n",
    "        lista_deurl.append(\"https://www.filmaffinity.com/es/film\" + peliIdValue + \".html\")\n",
    "        Lista_de_peliculas.append(nombrePeli)\n",
    "    except:\n",
    "        print(\"continue\")\n",
    "        escluidos.append(nombrePeli)\n",
    "    \n",
    "print(lista_deurl);\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    connection = pymysql.connect(host='localhost',user='root',password='Kojinanjo1@',db='pelis')\n",
    "    print(\"Conectado correctamente\")\n",
    "except pymysql.Error as e:\n",
    "    print(\"No puede conectar con MySQL %d: %s\" %(e.args[0], e.args[1]))\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "print(\"CONTINUE\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "titulos = []\n",
    "años = []\n",
    "duraciones = []\n",
    "paises= []\n",
    "direcciones= []\n",
    "guiones= []\n",
    "musicas= []\n",
    "fotografias= []\n",
    "repartos= []\n",
    "productoras= []\n",
    "generos= []\n",
    "grupos= []\n",
    "sinopsises= []\n",
    "\n",
    "\n",
    "# Crear un DataFrame\n",
    "\n",
    "for url in lista_deurl:\n",
    "    \n",
    "    print(url)\n",
    "    reparto = \"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    titulo = soup.find(\"dd\").text.strip()\n",
    "    año = soup.find(\"dd\", attrs = {\"itemprop\" : \"datePublished\"}).text\n",
    "    \n",
    "    duracionEl = soup.find(\"dd\", attrs = {\"itemprop\" : \"duration\"})\n",
    "    if duracionEl is None:\n",
    "        duracion = \"\"\n",
    "    else:\n",
    "        duracion = duracionEl.text\n",
    "        \n",
    "    print(duracion)\n",
    "\n",
    "    pais = soup.find(\"span\", attrs = {\"id\" : \"country-img\"}).find(\"img\")[\"alt\"]\n",
    "    \n",
    "    \n",
    "    if soup.find_all(\"div\", class_ = \"credits\")is None or len(soup.find_all(\"div\", class_ = \"credits\")) <= 3:\n",
    "        direccion =\"\"\n",
    "    else:\n",
    "        direccion =\"\".join(soup.find_all(\"div\", class_ = \"credits\")[2].text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if soup.find_all(\"div\", class_ = \"credits\")is None or len(soup.find_all(\"div\", class_ = \"credits\")) <= 3:\n",
    "        guion =\"\"\n",
    "    else:\n",
    "        guion =\"\".join(soup.find_all(\"div\", class_ = \"credits\")[2].text)    \n",
    "    \n",
    "\n",
    "    \n",
    "    if soup.find_all(\"div\", class_ = \"credits\") is None or len(soup.find_all(\"div\", class_ = \"credits\")) <= 3:\n",
    "        musica= \"\"\n",
    "    else:\n",
    "        musica = \" \".join(soup.find_all(\"div\", class_ = \"credits\")[3].text)\n",
    "    \n",
    "    \n",
    "       \n",
    "    fotografia = soup.find(\"div\", attrs = {\"id\" : \"movie-main-image-container\"}).find(\"img\")[\"src\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if soup.find_all(\"nb\", class_ = \"credits\") is None or len(soup.find_all(\"nb\", class_ = \"credits\")) <= 0:\n",
    "        reparto= \"\"\n",
    "    else:\n",
    "        reparto = \" \".join([t.text for t in soup.find(\"nb\", class_ = \"credits\").find_all(\"span\", class_ = \"nb\")])    \n",
    "        \n",
    "        \n",
    "    \n",
    "    if soup.find_all(\"dd\", class_ = \"card-producer\") is None or len(soup.find_all(\"dd\", class_ = \"card-producer\")) <= 0:\n",
    "        productora= \"\"\n",
    "    else:\n",
    "        productora = \" \".join([t.text for t in soup.find(\"dd\", class_ = \"card-producer\").find_all(\"span\", class_ = \"nb\")])    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    genero = \" \".join(soup.find(\"dd\", class_ = \"card-genres\").text.split())      \n",
    "        \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    # grupo = \" \".join(soup.find(\"dd\", attrs = {\"style\" : \"position: relative;\"}).text.split())\n",
    "    \n",
    "    if soup.find(\"dd\", attrs = {\"style\" : \"position: relative;\"}) is None:\n",
    "        grupo = \"\"\n",
    "    else:\n",
    "        grupo = \" \".join(soup.find(\"dd\", attrs = {\"style\" : \"position: relative;\"}).text.split())\n",
    "        \n",
    "        \n",
    "    if soup.find(\"dd\", attrs={\"itemprop\": \"description\"}):\n",
    "        sinopsis = soup.find(\"dd\", attrs={\"itemprop\": \"description\"}).text\n",
    "    else:\n",
    "        sinopsis = \"\"\n",
    "        \n",
    "        \n",
    "    titulos.append(titulo)\n",
    "    años.append(año)\n",
    "    duraciones.append(duracion)\n",
    "    paises.append(pais)\n",
    "    direcciones.append(direccion)\n",
    "    guiones.append(guion)\n",
    "    musicas.append(musica)\n",
    "    \n",
    "    \n",
    "    fotografias.append(fotografia)\n",
    "    repartos.append(reparto)\n",
    "    productoras.append(productora)\n",
    "    generos.append(genero)\n",
    "    grupos.append(grupo)\n",
    "    sinopsises.append(sinopsis)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "\n",
    "df[\"titulo\"] = [] if titulos is None else titulos  \n",
    "df[\"año\"] = [] if años is None else años\n",
    "df[\"duraciones\"] = [] if duraciones is None else duraciones\n",
    "df[\"paises\"] = [] if  paises is None else  paises\n",
    "df[\"direcciones\"] = [] if direcciones is None else  direcciones\n",
    "df[\"guiones\"] = [] if guiones is None else  guiones\n",
    "df[\"musicas\"] = [] if musicas is None else  musicas\n",
    "df[\"fotografias\"] = [] if fotografias is None else  fotografias\n",
    "df[\"repartos\"] = [] if repartos is None else repartos\n",
    "df[\"productoras\"]= [] if productoras is None else  productoras\n",
    "df[\"generos\"] = [] if generos is None else  generos\n",
    "df[\"grupos\"] = [] if grupos is None else  grupos\n",
    "df[\"sinopsises\"] = [] if sinopsises is None else  sinopsises\n",
    "\n",
    "\n",
    "try:\n",
    "    sql = \"\"\"INSERT INTO `pelis`.`pelis2` (`titulo`,`anio`,`duraciones`,`paises`,`direcciones`,`guiones`,\n",
    "`musicas`,`fotografias`,`repartos`,\n",
    "`productoras`,`generos`,`grupos`,`sinopsises`) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    print(sql)\n",
    "    for ind in df.index:\n",
    "        cursor.execute(sql, (df['titulo'][ind], df['año'][ind], df['duraciones'][ind], df['paises'][ind], \n",
    "                             df['direcciones'][ind], df['guiones'][ind], df['musicas'][ind], df['fotografias'][ind], \n",
    "                             df['repartos'][ind], df['productoras'][ind], df['generos'][ind], df['grupos'][ind], df['sinopsises'][ind] ))\n",
    "        print(df['titulo'][ind])\n",
    "        print(df['año'][ind])\n",
    "        print(df['duraciones'][ind])\n",
    "        print(df['paises'][ind])\n",
    "        print(df['direcciones'][ind])\n",
    "        print(df['guiones'][ind])\n",
    "        print(df['musicas'][ind])\n",
    "        print('Ahora van las fotos')\n",
    "        print(df['fotografias'][ind])\n",
    "        print('Ya acabaron las fotos')\n",
    "        print(df['repartos'][ind])\n",
    "        print(df['productoras'][ind])\n",
    "        print(df['generos'][ind])\n",
    "        print(df['grupos'][ind])\n",
    "        print(df['sinopsises'][ind])\n",
    "    connection.commit() \n",
    "except pymysql.Error as e:\n",
    "    print(\"Error BBDD %d: %s\" %(e.args[0], e.args[1]))\n",
    "\n",
    "\n",
    "print (escluidos)\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ebf4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a47759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
